#!/usr/bin/env python
# -*-coding:utf-8 -*-
'''
@File    :   data_util.py
@Time    :   2023/12/19 14:04:14
@Author  :   Zhiyuan Peng@Santa Clara University
@Version :   1.0
@Desc    :   general data processing, based on UPR
'''
import json
import logging
import torch
from typing import List
import random
from torch.nn.utils.rnn import pad_sequence
logger = logging.getLogger()


def read_data_from_json_file(path: str) -> List:
    with open(path, "r", encoding="utf-8") as f:
        logger.info("Reading file %s" % path)
        data = json.load(f)
    return data

def normalize_question(question: str) -> str:
    question = question.replace("â€™", "'")
    return question

class Dataset(torch.utils.data.Dataset):
    def __init__(
        self,
        special_token: str = None,
        shuffle_positives: bool = False,
        query_special_suffix: str = None,
        encoder_type: str = None,
    ):
        self.special_token = special_token
        self.encoder_type = encoder_type
        self.shuffle_positives = shuffle_positives
        self.query_special_suffix = query_special_suffix
        self.data = []

    def load_data(self, start_pos: int = -1, end_pos: int = -1):
        raise NotImplementedError

    def __len__(self):
        return len(self.data)

    def __getitem__(self, index):
        raise NotImplementedError

    def _process_query(self, query: str):
        # as of now, always normalize query
        query = normalize_question(query)
        if self.query_special_suffix and not query.endswith(self.query_special_suffix):
            query += self.query_special_suffix

        return query

def sample_data(cfg, data):
    """sample positive_ctxs, neg_ctxs according to cfg.pos_num, cfg.hard_neg_num and cfg.random_neg_num.
    for query q, if cfg.pos_num > current pos num, take all the pos, oterwise, take cfg.pos_num 
    cfg.hard_neg_num+cfg.random_neg_num = total neg num

    Args:
        cfg (_type_): _description_
        data (_type_): _description_

    Returns:
        _type_: _description_
    """
    positive_ctxs, neg_ctxs, queries = [], [], []
    # shuffle neg, gen pos and neg
    for i in range(len(data)):
        positive_passages = data[i].positive_passages
        neg_passages = data[i].negative_passages
        hard_neg_passages = data[i].hard_negative_passages
        if cfg.shuffle_neg:
            random.shuffle(neg_passages)
            random.shuffle(hard_neg_passages)
        if len(hard_neg_passages) == 0:
            hard_neg_passages = neg_passages[:cfg.hard_neg_num]
        else:
            hard_neg_passages = hard_neg_passages[:cfg.hard_neg_num]
        neg_passages = neg_passages[:cfg.random_neg_num]
        positive_ctxs.append(positive_passages[: min(cfg.pos_num, len(positive_passages))])
        neg_ctxs.append(neg_passages+hard_neg_passages)
        queries.append(data[i].query)
    return positive_ctxs, neg_ctxs, queries

def in_batch_neg(cfg, positive_ctxs, neg_ctxs, queries):
    """neg passages are generated by in batch neg method, only top 1 pos is selected

    Args:
        cfg (_type_): _description_
        positive_ctxs (_type_): _description_
        neg_ctxs (_type_): _description_
        queries (_type_): _description_

    Returns:
        _type_: _description_
    """
    # gen pairs, neg is genereated by in batch neg
    assert cfg.pos_num == 1, "In batch eng, pos_num must be 1"
    q_d_list = []
    for i in range(len(queries)):
        q_d_dict = {}
        q_d_dict["query"] = queries[i]
        # only one positive pair
        q_d_dict["pos"] = [" ".join([positive_ctxs[i][0].title+positive_ctxs[i][0].text]).strip()]
        neg_list = []
        for j in range(len(queries)):
            # the pos of other queries will be the neg for current query
            if j != i:
                neg_list.append(" ".join([positive_ctxs[j][0].title+positive_ctxs[j][0].text]).strip())
            # neg of all queries will be neg for current query
            for neg_ctx in neg_ctxs[j]:
                neg_list.append(neg_ctx.title + neg_ctx.text)
        q_d_dict["neg"] = neg_list
        q_d_list.append(q_d_dict)
    return q_d_list

def non_in_batch_neg(cfg, positive_ctxs, neg_ctxs, queries):
    """non in batch neg method. All the pos and neg docs are utilized

    Args:
        cfg (_type_): _description_
        positive_ctxs (_type_): _description_
        neg_ctxs (_type_): _description_
        queries (_type_): _description_

    Returns:
        _type_: _description_
    """
    # gen pairs, neg is genereated by in batch neg
    assert len(positive_ctxs) == len(neg_ctxs)
    assert len(neg_ctxs) == len(queries)
    assert len(queries) == cfg.q_num_per_batch
    q_d_list = []
    for i in range(len(queries)):
        q_d_dict = {}
        q_d_dict["query"] = queries[i]
        q_d_dict["pos"] = [" ".join([positive_ctxs[i][j].title+positive_ctxs[i][j].text]).strip() for j in range(len(positive_ctxs[i]))]
        # in this mode, all the pos are selected, because of the drop last, shuffle the poss queries, so that drop last will not always drop certain pos queries
        random.shuffle(q_d_dict["pos"])
        q_d_dict["neg"] = [" ".join([neg_ctxs[i][j].title+neg_ctxs[i][j].text]).strip() for j in range(len(neg_ctxs[i]))]
        q_d_list.append(q_d_dict)
    return q_d_list

def pad_batch(cfg, inputs, targets):
    """pad inputs and tagets

    Args:
        inputs (_type_): _description_
        targets (_type_): _description_

    Returns:
        _type_: _description_
    """
    assert len(inputs) == len(targets)
    model_inputs = cfg.tokenizer(inputs)
    model_inputs["input_ids"] = [row[1:] for row in model_inputs["input_ids"]]
    labels = cfg.tokenizer(targets)
    # if dynamic max len < cfg.max_seq_len, then update the max_len as dynamic max len to save GPU
    dynamic_input_length = 0
    dynamic_label_length = 0
    for i in model_inputs["input_ids"]:
        if len(i) > dynamic_input_length:
            dynamic_input_length = len(i)
    for i in labels["input_ids"]:
        if len(i) > dynamic_label_length:
            dynamic_label_length = len(i)
    # append one pad token to label: prompt+label+1
    max_length = dynamic_input_length + dynamic_label_length + 1
    if max_length >= cfg.max_seq_len:
        max_length = cfg.max_seq_len
    # define doc_specific
    doc_input_ids, doc_label_ids, doc_mask_ids = [], [], []
    # padding
    for i in range(len(inputs)):
        # concatenate text with label
        sample_input_ids = model_inputs["input_ids"][i]
        label_input_ids = labels["input_ids"][i] + [cfg.tokenizer.pad_token_id]
        label_len = len(label_input_ids)
        model_inputs["input_ids"][i] = sample_input_ids + label_input_ids
        labels["input_ids"][i] = [-100] * len(sample_input_ids) + label_input_ids
        model_inputs["attention_mask"][i] = [1] * len(model_inputs["input_ids"][i])
        # pad to max_length
        if max_length > len(model_inputs["input_ids"][i]):
            model_inputs["input_ids"][i] = [cfg.tokenizer.pad_token_id] * (max_length - len(model_inputs["input_ids"][i])) + model_inputs["input_ids"][i]
            model_inputs["attention_mask"][i] = [0] * (max_length - len(model_inputs["attention_mask"][i])) + model_inputs["attention_mask"][i]
            labels["input_ids"][i] = [-100] * (max_length - len(labels["input_ids"][i])) + labels["input_ids"][i]
        elif max_length < len(model_inputs["input_ids"][i]):
            # only cut the document, keep the start token id 1
            model_inputs["input_ids"][i] = [model_inputs["input_ids"][i][0]]+model_inputs["input_ids"][i][-(max_length-1):]
            model_inputs["attention_mask"][i] = [model_inputs["attention_mask"][i][0]] + model_inputs["attention_mask"][i][-(max_length-1):]
            labels["input_ids"][i] = [labels["input_ids"][i][0]] + labels["input_ids"][i][-(max_length-1):]
        model_inputs["input_ids"][i] = torch.tensor(model_inputs["input_ids"][i])
        model_inputs["attention_mask"][i] = torch.tensor(model_inputs["attention_mask"][i])
        labels["input_ids"][i] = torch.tensor(labels["input_ids"][i])
        # for doc-specific
        doc_input_ids.append(model_inputs["input_ids"][i][-label_len+1:-1])
        doc_label_ids.append(torch.full_like(labels["input_ids"][i][-label_len+1:-1], -100))
        doc_mask_ids.append(model_inputs["attention_mask"][i][-label_len+1:-1])
        #
    model_inputs["input_ids"] = torch.vstack(model_inputs["input_ids"])
    model_inputs["attention_mask"] = torch.vstack(model_inputs["attention_mask"])
    labels["input_ids"] = torch.vstack(labels["input_ids"])
    # doc_specific
    doc_input_ids = pad_sequence(doc_input_ids, padding_value=cfg.tokenizer.pad_token_id, batch_first=True)
    doc_label_ids = pad_sequence(doc_label_ids, padding_value=-100, batch_first=True)
    doc_mask_ids = pad_sequence(doc_mask_ids, padding_value=0, batch_first=True)
    doc_specific = {"input_ids": doc_input_ids, "attention_mask": doc_mask_ids, "labels": doc_label_ids}
    if cfg.doc_specific:
        return {"input_ids": model_inputs["input_ids"], "attention_mask": model_inputs["attention_mask"], "labels": labels["input_ids"], "doc_specific": doc_specific}
    else:
        return {"input_ids": model_inputs["input_ids"], "attention_mask": model_inputs["attention_mask"], "labels": labels["input_ids"], "doc_specific": None}

def pointwise_collate_func(cfg, data):
    """only generate pos doc-query pairs, no neg doc-query pairs

    Args:
        cfg (_type_): _description_
        data (_type_): _description_

    Yields:
        _type_: a generator, each iteration, generate cfg.batch_size data points
    """
    positive_ctxs, neg_ctxs, queries = sample_data(cfg, data)
    q_d_list = non_in_batch_neg(cfg, positive_ctxs, neg_ctxs, queries)
    # extract datapoint from tuples
    inputs = []
    targets = []            
    # add pos
    for q_d_dict in q_d_list:
        for pos_doc in q_d_dict["pos"]:
            if cfg.fixed_prompt:
                inputs.append("{} \n Passage: {} \n Question: \n".format(cfg.fixed_prompt, pos_doc))
            else:
                inputs.append("Passage: {} \n Question: \n".format(pos_doc))
            targets.append(q_d_dict["query"])
    assert len(targets) == len(inputs)
    # shuffle the data
    c = list(zip(inputs, targets))
    random.shuffle(c)
    inputs, targets = zip(*c)
    # remove the left data points, so that total_point_num can be divided by cfg.batch_size
    assert cfg.batch_size <= cfg.q_num_per_batch
    total_point_num = len(inputs)
    total_point_num -= (total_point_num % cfg.batch_size)
    inputs = inputs[:total_point_num]
    targets = targets[:total_point_num]
    # pad by batch_size
    return [pad_batch(cfg, inputs[i: i+cfg.batch_size], targets[i: i+cfg.batch_size]) for i in range(0, len(inputs), cfg.batch_size)]
    
def pairwise_collate_func(cfg, data):
    """gen batch_size=2 data, the first one is doc-pos_query, the second is doc-neg_query

    Args:
        cfg (_type_): _description_
        data (_type_): _description_

    Yields:
        _type_: a list of batch_size doc-query pairs, batch_size is a even number. Here is an exp:
        q1's posDoc1-q1
        q1's negDoc5-q1
        q2's posDoc2-q2
        q2's negDoc10-q2
        to compute loss, in each batch, 1 and 2, 3 and 4, ... are pairs: loss(1, 2)+loss(3, 4)
    """
    assert cfg.batch_size % 2 == 0
    positive_ctxs, neg_ctxs, queries = sample_data(cfg, data)
    if cfg.disable_in_batch_neg:
        q_d_list = non_in_batch_neg(cfg, positive_ctxs, neg_ctxs, queries)
    else:
        q_d_list = in_batch_neg(cfg, positive_ctxs, neg_ctxs, queries)
    # extract datapoint from tuples
    inputs = []
    targets = []            
    for q_d_dict in q_d_list:
        # for each pos, iterate all neg docs, to create a pair for each pos and neg
        for pos in q_d_dict["pos"]:
            for neg in q_d_dict["neg"]:
                # pair each neg with a pos
                if cfg.fixed_prompt:
                    inputs.append("{} \n Passage: {} \n Question: \n".format(cfg.fixed_prompt, pos))
                    targets.append(q_d_dict["query"])
                    inputs.append("{} \n Passage: {} \n Question: \n".format(cfg.fixed_prompt, neg))
                else:
                    inputs.append("Passage: {} \n Question: \n".format(pos))
                    targets.append(q_d_dict["query"])
                    inputs.append("Passage: {} \n Question: \n".format(neg))
                targets.append(q_d_dict["query"])
    total_point_num = len(inputs)
    # drop the last batch if len < cfg.batch_size
    total_point_num -= (total_point_num % cfg.batch_size)
    inputs = inputs[:total_point_num]
    targets = targets[:total_point_num]
    # pad by batch_size
    return [pad_batch(cfg, inputs[i: i+cfg.batch_size], targets[i: i+cfg.batch_size]) for i in range(0, len(inputs), cfg.batch_size)]

def listwise_collate_func():
    pass

def infer_collate_func(cfg, data):
    """for each question, pair it with its items for testing

    Args:
        cfg (_type_): _description_
        data (_type_): _description_

    Returns:
        _type_: a list of samples:
        sample = {'id': idx,
                  'question': question,
                  'answers': answers,
                  'items': items}
    """
    inputs = []
    answers = []
    ori_items = []
    legal_samples = []            
    # add pos
    for sample in data:
        mini_inputs, mini_targets, mini_answers = [], [], []
        query = sample["question"]
        items = sample["items"]
        if cfg.test_clean:
            item_legal = False
            for item in items: 
                if item["has_answer"]:
                    item_legal = True
                    break
            if not item_legal:
                continue
        # return legal sample
        legal_samples.append(sample)
        for item in items: 
            doc = item["text"] 
            mini_answers.append(item["has_answer"])
            if cfg.fixed_prompt:
                mini_inputs.append("{} \n Passage: {} \n Question: \n".format(cfg.fixed_prompt, doc))
            else:
                mini_inputs.append("Passage: {} \n Question: \n".format(doc))
            mini_targets.append(query)
        assert len(mini_targets) == len(mini_inputs)
        assert len(mini_targets) == len(mini_answers)
        # split into batches
        inputs.append([pad_batch(cfg, mini_inputs[i: i+cfg.batch_size], mini_targets[i: i+cfg.batch_size]) for i in range(0, len(items), cfg.batch_size)])
        answers.append([mini_answers[i: i+cfg.batch_size] for i in range(0, len(items), cfg.batch_size)])
        ori_items.append([items[i: i+cfg.batch_size] for i in range(0, len(items), cfg.batch_size)])
    return inputs, answers, ori_items, legal_samples